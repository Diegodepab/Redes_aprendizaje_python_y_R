---
title: "`Tarea_1:` Analisís de la expresión Génica usando Deep Learning"
subtitle: "Herramientas y Algoritmos en Bioinformática"
author: "Diego de Pablo, estudiante de Bioinformática, Universidad de Málaga"
profesor: "Montes Torres Julio"
date: "`r format(Sys.time(), '%A %d, %B %Y')`"
output: 
  html_document:
    theme: spacelab
    number_sections: true
    toc: true
    toc_float: true
    code_folding: "hide"
    fontsize: 12pt
editor_options: 
  markdown: 
    wrap: sentence
---

```{=html}
<style>
/* Estilos generales */
h1, h2, h3 {
    color: #2E8B57; /* Color de los títulos - verde bosque */
    font-family: 'Arial', sans-serif; /* Fuente de los títulos */
    font-weight: bold; /* Negrita para los títulos */
}
  /* Tamaños específicos de los títulos */
  h1 {
      font-size: 20px; /* Tamaño más pequeño para h1 */
  }
  h2 {
      font-size: 18px; /* Tamaño más pequeño para h2 */
  }
  h3 {
      font-size: 16px; /* Tamaño más pequeño para h3 */
  }
  
/* Párrafos y texto normal */
p {
    color: #000000; /* Asegura que el texto de los párrafos sea negro */
    line-height: 1.6; /* Aumenta el espacio entre líneas para mejor legibilidad */
}

/* Estilos personalizados para detalles */
.custom-details {
    border: 2px solid #32CD32; /* Borde verde */
    border-radius: 8px;
    background-color: #f0fff0; /* Fondo verde claro */
    padding: 15px;
    margin: 15px 0;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Sombra suave */
}

.custom-details summary {
    font-weight: bold;
    color: #32CD32; /* Color del texto verde */
    cursor: pointer;
}

/* Estilos para el recuadro verde */
.green-alert {
    border: 1px solid #32CD32; /* Borde verde */
    background-color: #f0fff0; /* Fondo verde claro */
    color: #006400; /* Color del texto verde oscuro */
    padding: 15px;
    border-radius: 5px;
    margin-bottom: 20px;
    font-size: 15px;
}

</style>
```
*Este proyecto tiene como objetivo comprender diversas herramienas que permiten entrenar una red de aprendizaje profundo a través del marco H2O, con el fin de predecir el estado de pacientes de cáncer de riñón (fallecidos vs. vivos). Se utiliza un conjunto de datos con 267 casos de pacientes fallecidos y 753 controles vivos, con más de 20,000 genes por paciente. Además se usa la función glmnet para evaluar un modelo de regresión LASSO con las mismas características que se seleccionaron para la red de aprendizaje profundo y se contrastaron los resultados.*

# Preparación del trabajo

Este trabajo se podría dividir en 3 etapas, la primera referente a la preparación del código principal declarando aquellas funciones necesarias como las métricas de evaluacion para comenzar el desarrollo de una red de aprendizaje profundo, luego vendría el código principal que es el propio entrenamiento de la red de aprendizaje y por último contrastar los resultados obtenidos con un modelo de regresión LASSO.

```{r inicialización, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Limpiar el entorno de trabajo
rm(list = ls())  # Elimina todos los objetos en el entorno global
gc()  # Liberar memoria no utilizada

# Función para verificar si una librería está instalada, si no lo está, la instala
instalar_paquete <- function(paquete) {
  if (!requireNamespace(paquete, quietly = TRUE)) {
    install.packages(paquete, dependencies = TRUE)
  }
  library(paquete, character.only = TRUE)
}

# Lista de librerías necesarias para esta entrega
librerias <- c("pROC", "h2o", "uuid", "caret", "glmnet")

# Aplicamos la función a cada librería
invisible(lapply(librerias, instalar_paquete))

# Configuración de knitr
knitr::opts_chunk$set(echo = TRUE)

```


## Evaluación de Modelos de Clasificación

A pesar de que es el inicio del trabajo se debe empezar definiendo las métricas que se usarán para evaluar el rendimiento de cada modelo entrenado, se evalúa utilizando métricas de clasificación como precisión y AUC para los conjuntos de entrenamiento y prueba.
Estas métricas se calculan a través de la función `get.classification.measures()`.

```{r model-evaluation-measures}
get.classification.measures <- function(true.class, pred.probs) {
     
     true.class <- as.numeric(true.class) # convert FALSE/TRUE to 0/1
     pred.class <- as.numeric(pred.probs > 0.5)
     
     cases.idx         <- which(true.class == 1)
     controls.idx      <- which(true.class == 0)
     
     res <- data.frame("accuracy"=0)
     
     # Accuracy
     res$accuracy <- sum(true.class == pred.class) / length(true.class)
     
     # Area under the Receiver-Operator Curve and Confidence Intervals
     res$AUC <- as.numeric(pROC::auc(response = true.class, predictor = pred.probs))
     
     return(unlist(res))
}
```

Los valores de AUC y precisión se calculan para cada conjunto de entrenamiento y prueba, y se almacenan en una matriz llamada `perfs.mat`.
Esta matriz se utiliza para almacenar los resultados de cada repetición de la validación cruzada.
Donde aquellos resultados más cercanos a 1 indicarán mejores resultados.

## Entrenamiento y Optimización de Redes Neuronales Profundas con H2O: Búsqueda de Parámetros Óptimos

En este bloque de código, se presenta una función `deepnet.training` diseñada para entrenar una red neuronal profunda utilizando la librería H2O.
La función comienza separando los datos en un conjunto de entrenamiento y validación, luego define un rango de parámetros que serán probados para encontrar la configuración óptima del modelo.
La búsqueda de parámetros se realiza de manera *aleatoria*, probando diferentes combinaciones de parámetros de activación, tasa de aprendizaje, dropout y otras configuraciones.
Finalmente, el modelo que obtenga el mejor desempeño en el conjunto de validación se selecciona para entrenar un modelo final utilizando todos los datos de entrenamiento disponibles.

```{r busqueda_param}
deepnet.training <- function(x, y, inner.folds) {
     y <- plyr::revalue(factor(y), c("0"="no", "1"="yes"))
     
     my.uuid <- gsub("-", "", UUIDgenerate())
     
     #split train in 80% train and 20% validation
     train.ids = sort(sample(1:nrow(x), size = floor(nrow(x)*0.8)))
     val.ids   = setdiff(1:nrow(x), train.ids)
     
     X.train = x[train.ids,]
     X.val   = x[val.ids,]
     Y.train = y[train.ids]
     Y.val   = y[val.ids]
     
     data.train <- cbind(outcome=Y.train, as.data.frame(X.train))
     data.train <- h2o::as.h2o(data.train, paste0("data.train.",my.uuid))
     data.val   <- cbind(outcome=Y.val, as.data.frame(X.val))
     data.val   <- h2o::as.h2o(data.val, paste0("data.val.",my.uuid))
     
     #deepnet parameters to try
     rand_activation     <- RAND.ACTIVATION
     rand_rho            <- seq(0.9, 0.99, 1e-3)
     rand_epsilon        <- c(1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4)
     rand_input_dropout  <- seq(RAND.MIN.INPUT.DROPOUT, RAND.MAX.INPUT.DROPOUT, 1e-4)
     rand_l1             <- seq(RAND.MIN.L1, RAND.MAX.L1, 1e-4)
     rand_l2             <- seq(RAND.MIN.L2, RAND.MAX.L2, 1e-4)
     
     RAND.MAX.NEURONS.PER.LAYER = min(RAND.MAX.NEURONS.PER.LAYER, ncol(x))
     MAX.RUNTIME.SECS = MAX.RUNTIME.SECS/(RAND.MAX.NUM.HIDDEN.LAYERS-RAND.MIN.NUM.HIDDEN.LAYERS+1)
     bestgrids = list()
     for (netsize in RAND.MIN.NUM.HIDDEN.LAYERS:RAND.MAX.NUM.HIDDEN.LAYERS) {
          rand_hidden         <- lapply(lapply(1:500,
                                               function(x) RAND.MIN.NEURONS.PER.LAYER+sample(RAND.MAX.NEURONS.PER.LAYER-RAND.MIN.NEURONS.PER.LAYER, netsize, replace=F)),
                                        function(x) sort(x, decreasing = T))
          rand_hidden_dropout <- lapply(lapply(1:500,
                                               function (x) sample(seq(RAND.MIN.HIDDEN.DROPOUT, RAND.MAX.HIDDEN.DROPOUT, 1e-4), netsize, replace = F)),
                                        function(x) sort(x, decreasing = T))
          
          hyper_params <- list(activation = rand_activation, rho = rand_rho, epsilon = rand_epsilon,
                               hidden = rand_hidden, input_dropout_ratio = rand_input_dropout, hidden_dropout_ratios = rand_hidden_dropout,
                               l1 = rand_l1, l2 = rand_l2)
          search_criteria = list(strategy = "RandomDiscrete",
                                 max_models = NUM.RANDOM.TRIALS, max_runtime_secs = MAX.RUNTIME.SECS,
                                 seed=123456)
          
          model_grid <- h2o.grid("deeplearning",
                                 grid_id = paste0("gridsize.",netsize,".",my.uuid),
                                 hyper_params = hyper_params,
                                 search_criteria = search_criteria,
                                 x = colnames(x),
                                 y = "outcome",
                                 training_frame = data.train,
                                 validation_frame = data.val,
                                 balance_classes = T,
                                 epochs = TRIAL.EPOCHS,
                                 stopping_rounds = 3,
                                 stopping_tolerance = 0.02,
                                 stopping_metric = "AUC")
          
          aucs.train.perf = c()
          aucs.val.perf   = c()
          for (mi in model_grid@model_ids) {
               aucs.train.perf = c(aucs.train.perf, h2o.auc(h2o.getModel(mi)))
               aucs.val.perf   = c(aucs.val.perf, h2o.auc(h2o.getModel(mi), valid = T))
          }
          
          #
          bestgrids[[paste0("size",netsize)]] = h2o.getModel(model_grid@model_ids[[which.max(aucs.val.perf)]])
     }
     
     #grab best deepnet and use its parameter to fit a final deepnet to the complete train set
     best.uuid <- gsub("-", "", UUIDgenerate())
     best.model.tried = bestgrids[[which.max(sapply(bestgrids, function(x) h2o.auc(x, valid = T)))]]
     
     data <- cbind(outcome=y, as.data.frame(x))
     data <- h2o::as.h2o(data, paste0("data.",best.uuid))
     
     model <- h2o.deeplearning(x=colnames(x), y="outcome",
                               training_frame = data, model_id = paste0("bestmodel.",best.uuid),
                               activation = best.model.tried@parameters$activation,
                               hidden = best.model.tried@parameters$hidden,
                               epochs = EPOCHS,
                               rho = best.model.tried@parameters$rho,
                               epsilon = best.model.tried@parameters$epsilon,
                               input_dropout_ratio = best.model.tried@parameters$input_dropout_ratio,
                               hidden_dropout_ratios = best.model.tried@parameters$hidden_dropout_ratios,
                               l1 = best.model.tried@parameters$l1,
                               l2 = best.model.tried@parameters$l2,
                               stopping_rounds = 3,
                               stopping_tolerance = 0.02,
                               stopping_metric = "AUC",
                               balance_classes = T,
                               export_weights_and_biases = T
     )
     
     return(list(id=model@model_id, deepnet=model,
                 parameters=unlist(model@allparameters[c("activation","rho","epsilon","hidden","epochs","input_dropout_ratio","hidden_dropout_ratios","l1","l2")])))
}
```

## Predicción con Red Neuronal Profunda Utilizando H2O

Este bloque de código define la función deepnet.predictions, que se encarga de realizar predicciones utilizando un modelo de red neuronal profunda previamente entrenado con H2O.
La función convierte los datos de entrada en un formato compatible con H2O, realiza la predicción, y extrae los resultados en forma de probabilidades.
Se asegura de que no se generen probabilidades negativas, lo cual indicaría un problema con la configuración del modelo.

```{r prediccion_red}
deepnet.predictions <- function(model, x) {
     my.uuid     <- gsub("-", "", UUIDgenerate())
     newdata.hex <- h2o::as.h2o(as.data.frame(x), paste0("newdata.",my.uuid))
     predictions <- as.numeric(as.matrix(predict(model$deepnet, newdata = newdata.hex)[,"yes"]))
     h2o::h2o.rm(paste0("newdata.",my.uuid))
     gc()
     
     if (any(predictions<0)) stop("Negative probabilities predicted... Check the configuration of the trained deepnet")
          
     return(predictions)
}
```

## Prueba `t de Student` para Comparación de Medias

La función `univar.ttest` realiza una prueba t de Student para comparar las medias de dos grupos: controles (cuando *y == 0)* y casos (cuando *y == 1*).
La función devuelve el valor p asociado a la prueba, que indica si las medias de los dos grupos son estadísticamente diferentes.

```{r t_student}
univar.ttest <- function(x, y) {
     controls = x[y==0]
     cases = x[y==1]
     
     return(t.test(controls, cases)$p.value)
}
```

## Reducción de Características Usando Prueba t y Correlación

La `función ttest`.feature.reduction selecciona características relevantes usando la prueba t para comparar dos grupos y luego reduce la dimensionalidad eliminando características altamente correlacionadas hasta un número máximo definido.

```{r}
ttest.feature.reduction <- function(myX, myY, pval.thres = 0.05, max.vars = 200) {
     pvals = apply(myX, 2, function(x, y) univar.ttest(x,y), myY)
     
     myX = myX[, pvals < pval.thres]
     
     cutoff = 0.95
     while (ncol(myX)>max.vars) {
          cormatrix = stats::cor(as.matrix(myX))
          hc = findCorrelation(abs(cormatrix), cutoff=cutoff) # putt any value as a "cutoff"
          if (length(hc)>0){
               hc = sort(hc)
               myX = myX[,-c(hc)]
          }
          cutoff = cutoff-0.05
     }
     
     return(colnames(myX))
}


```

# Entrenamiento de la Red Neuronal Profunda



Este código realiza un proceso de validación cruzada (10-fold CV) sobre un conjunto de datos para entrenar (extraídos desde Rdata) y evaluar una red neuronal profunda (DeepNet), utilizando un enfoque de selección de características y entrenando el modelo en un entorno distribuido utilizando **H2O**. 

Para fácilitar el entendimiento y observar de mejor manera en que partes del código se tardan más además de poder separar las partes que interesa modificar para mejorar el rendimiento, se ha dividido en secciones que se explican a continuación:

## Parámetros de Filtrado de Datos

Estas variables controlan la selección de las características que se utilizarán en el modelo, lo que puede mejorar la eficiencia y precisión del entrenamiento.

```{r filtrado}
DATABASE = "KIPAN"
REP.INIT = 1 #starting repetition id for several repetitions of 10-fold-CV
REP.END  = 1 #ending repetition id for several repetitions of 10-fold-CV

DATASET.FILE     = "KIPAN__illuminahiseq_rnaseqv2__Level_3__RSEM_genes_normalized.data.RData"
SAVE.RDATA.FILE  = paste0("ttestcor-deepnet_rep_", REP.INIT, "_", REP.END, ".RData")
SAVE.CSV.FILE    = paste0("ttestcor-deepnet_rep_", REP.INIT, "_", REP.END, ".csv")

#Filtering parameters
PVAL.THRES = 0.0001
MAX.VARS = 125
```

- **`REP.END`**: Se intento aumentar el número de repeticiones del 10-fold-CV (de `1` a `10`) para mejorar la robustez de los resultados, pero esto aumento demasiado el tiempo de ejecución, lo cual termino siendo inviable. Terminando el valor en 1.

- **`PVAL.THRES`**: Umbral de p-valor para la selección de características. Reducir el umbral (por ejemplo, de `0.001` a `0.0001`) podría seleccionar menos características, pero más relevantes. A su vez, esto reduce el tiempo de entrenamiento.
- **`MAX.VARS`**: Número máximo de características a retener después de la selección de características. Ajustar este valor (por ejemplo, cambiar de `270` a `125`) puede ayudar a reducir la dimensionalidad del modelo, mejorando la velocidad de entrenamiento y evitando el sobreajuste.

## Parámetros de la Red Neuronal

Son fundamentales para la definición y optimización de la red neuronal, afectando tanto los resultados como la eficiencia computacional.

```{r param}
#DeepNets parameters
NUM.RANDOM.TRIALS = 500
MAX.RUNTIME.SECS = 600  # per fold, maximum spend 10 minutes trying different deepnets to select the best one => 100 minutes to run 10-fold-CV
TRIAL.EPOCHS = 100
EPOCHS = 200 #originalmente 2000
RAND.ACTIVATION = c("RectifierWithDropout","TanhWithDropout","MaxoutWithDropout")
RAND.MIN.NUM.HIDDEN.LAYERS  = 2
RAND.MAX.NUM.HIDDEN.LAYERS  = 4
RAND.MIN.NEURONS.PER.LAYER  = 10
RAND.MAX.NEURONS.PER.LAYER  = 200
RAND.MIN.INPUT.DROPOUT      = 1e-2
RAND.MAX.INPUT.DROPOUT      = 0.1
RAND.MIN.HIDDEN.DROPOUT     = 1e-2
RAND.MAX.HIDDEN.DROPOUT     = 0.1
RAND.MIN.L1                 = 1e-2
RAND.MAX.L1                 = 0.1
RAND.MIN.L2                 = 1e-2
RAND.MAX.L2                 = 0.1
```

- **`NUM.RANDOM.TRIALS`**: Número de intentos aleatorios para ajustar los hiperparámetros de la red. Se intento reducir el número de intentos (de `500` a `250`) para disminuir el ritmo de entrenamiento, aunque a diferencia de los otros puntos el reducir la cantidad de intentos redujo significativamente los resultados (por lo cual no se termino dejando este cambio).

- **`EPOCHS`**: Número de épocas o iteraciones completas a través de los datos de entrenamiento, para entrenar la red. Reducir el número de épocas (de `2000` a `200`) puede acelerar el entrenamiento, aunque podría limitar la capacidad del modelo para aprender patrones complejos.

- **`MAX.RUNTIME.SECS`**: Tiempo máximo permitido para entrenar cada modelo. Similar al `NUM.RANDOM.TRIALS` disminuir este valor (de `600` a `100`) puede ayudar a reducir la carga computacional pero limito bastante la convergencia de los modelos, haciendolo infactible(por lo cual no se termino dejando este cambio).

- **`RAND.MIN.NEURONS.PER.LAYER`** y **`RAND.MAX.NEURONS.PER.LAYER`**: Rango de neuronas por capa en la red. Ajustar el número de neuronas (de `10-200` a `20-100`) puede equilibrar el desempeño del modelo con la carga computacional. Reducir el número de neuronas puede acelerar el entrenamiento pero disminuir la capacidad del modelo para aprender patrones complejos.

- **`RAND.MIN.L1`** y **`RAND.MAX.L1`**: Rango de regularización L1 para evitar sobreajuste. Incrementar la regularización L1 ( de `1e-3` a `1e-2`) puede ayudar a mejorar la generalización y reducir el riesgo de sobreajuste, aunque podría ralentizar ligeramente el entrenamiento.

## Parámetros de H2O

Estos parámetros controlan la configuración del servidor H2O, influyendo en el uso de recursos y la paralelización del proceso de entrenamiento.

```{r h2o_param}
# H2O parameters
nthreads                    = 6 # En mi caso tengo un Intel Core i7-1165G7 con 4 núcleos y 8 hilos, quiero mantener el portatil en buen estado y como no tengo prisa en los resultados, pongo 6 hilos para no causarle estrés mayor a la máquina, usando -1 en nhtreads se usan todos los hilos disponibles
max_mem_size                = "12g" # En mi caso tengo 16 GB de RAM, por lo que pongo 10 GB para no saturar la memoria

```

- **`nthreads`**: Número de hilos de CPU utilizados por H2O. en este caso se decide usar 6 en vez del máximo para evitar sobrecargar el sistema y permitir el uso de otros recursos mientras se entrena el modelo.

- **`max_mem_size`**: Tamaño máximo de memoria RAM asignada al servidor H2O.

## Carga y evaluación de los datos

Este código está cargando un conjunto de datos, configurando el entorno para entrenamiento de un modelo de Machine Learning y preparando los datos para su normalización y evaluación. 

```{r carga_datos}

#cargamos los datos
load(DATASET.FILE)

set.seed(69)

# Number of observations
N = nrow(datainfo$data)
# Number of predictors
P = (ncol(datainfo$data)-1)

print(paste("#samples:", N))
print(paste("#variables:", P))

X = datainfo$data[,-1]
y = datainfo$data[,1]

#normalize the data
#to avoid problems with memory, the normalization is make in two steps
X[,1:floor(P/2)] <- scale(X[,1:floor(P/2)])
X[,(floor(P/2)+1):P] <- scale(X[,(floor(P/2)+1):P])

simul <- NULL
perfs.mat <- matrix(NA, nrow=10*((REP.END-REP.INIT)+1), ncol=5)
colnames(perfs.mat) <- c("repetition","acc.train","auc.train","acc.test","auc.test")
right.row=1

```

Se puede apreciar como de los 267 casos (fallecidos) y 753 controles (vivos) se obtienen 1020 muestras que cada una tiene 20144 genes representado en variables, lo que hace que el conjunto de datos sea muy grande y complejo.


## Entrenamiento de Red Neuronal Profunda con Validación Cruzada y Selección de Características Univariantes

Se realiza un ciclo de validación cruzada (10-fold cross-validation) sobre varios repeticiones del experimento, donde se entrenan redes neuronales profundas utilizando la biblioteca h2o. En cada repetición, se aplica una selección de características univariantes (basada en pruebas t) para reducir las variables a las más relevantes, seguida de un proceso de ajuste de parámetros con validación interna. Luego, se entrena un modelo de red neuronal profunda utilizando las características retenidas y se evalúa su desempeño en los conjuntos de entrenamiento y prueba. Los resultados de rendimiento, junto con las características seleccionadas y los parámetros del modelo, se guardan para su posterior análisis.


```{r entrenamiento_profundo, warning=FALSE}
# Registra el tiempo antes de compilar el archivo RMarkdown
start_time <- Sys.time()



# loop over several repetitions of cross-validation
for (rep in seq(REP.INIT, REP.END)) {
     cat(paste0("Repetition ", rep, "\n\n"))
     
     #the following line obtains the folds for 10-fold CV;
     folds <- datainfo$folds[datainfo$folds[,1]==rep, 2:ncol(datainfo$folds)]
     
     num.outter.folds <- ncol(folds)
     iter.res = NULL
     start.time = Sys.time()
     #loop over different folds
     for (ff in 1:num.outter.folds) {
          cat(paste0("CV ", ff, "-fold\n"))
          
          #obtain the training and test ids and their respective subsets
          train.ids   <- which(folds[, ff] != -1)
          test.ids    <- which(folds[, ff] == -1)
          X.train     <- X[train.ids,]
          X.test      <- X[test.ids,]
          Y.train     <- y[train.ids]
          Y.test      <- y[test.ids]
          
          # inner folds to be used in glmnet to learn lambda
          inner.folds <- folds[train.ids, ff]
          
          
          ########################################################################################
          # ANY FS PROCEDURE, CHI-SQUARED OR ANY OTHER HERE, in this a univariate feature selection with
          # multiple testing correction followed by a procedure that gets rid of highly correlated genes
          # in case more than 200 genes were retained
          retained.features = ttest.feature.reduction(X.train, Y.train, pval.thres = PVAL.THRES, max.vars = MAX.VARS)
          print(paste0(length(retained.features), " genes retained..."))
          X.train = X.train[,retained.features,drop=FALSE]
          X.test  = X.test[,retained.features,drop=FALSE]
          
          ########################################################################################
          # AND NOW FIT A MODEL (LASSO, DEEPNETS, OR ANY OTHER) WITH THE RETAINED FEATURES
          # In this case, train a deep net
          #start (or connect) to h2o server
          h2o.init(nthreads = -1, max_mem_size = "2G", ip = "127.0.0.1")

          Sys.sleep(2)

          model <- deepnet.training(x=X.train, y=Y.train, inner.folds=inner.folds)
          print(model$parameters)

          ########################################################################################

          #number of optimal predictors selected by lasso
          iter.res[[paste0("cvfold",ff)]][["retained.predictors"]] <- retained.features
          iter.res[[paste0("cvfold",ff)]][["deepnet.parameters"]]  <- model$parameters

          #compute train predictions over the train set, removing the outcome since we don't know it
          train.predictions <- deepnet.predictions(model = model, x = X.train)
          #compute test predictions over the test set, removing the outcome since we don't know it
          test.predictions <- deepnet.predictions(model = model, x = X.test)


          #compute classification measures within this test-fold
          train.auc <- get.classification.measures(true.class = Y.train, pred.probs = train.predictions)
          test.auc  <- get.classification.measures(true.class = Y.test, pred.probs = test.predictions)

          iter.res[[paste0("cvfold",ff)]][["train.perf"]] <- train.auc
          iter.res[[paste0("cvfold",ff)]][["test.perf"]]  <- test.auc

          perfs.mat[right.row,] <- c(rep, train.auc, test.auc)
          print(perfs.mat[right.row,])
          right.row = right.row+1

          h2o.shutdown(FALSE)
          Sys.sleep(2)

          write.csv(perfs.mat, file=SAVE.CSV.FILE, quote=F, row.names=F)
     } #end ff
     
     time.in.mins = as.numeric(difftime(Sys.time(), start.time, units="mins"))
     simul[[paste0("repetition",rep)]] = list(folds=iter.res, time.in.mins=time.in.mins)
     
     save(simul, file=SAVE.RDATA.FILE, compress="xz")
} #end rep


# Registra el tiempo después de la compilación
end_time <- Sys.time()

# Calcula y muestra el tiempo de ejecución
execution_time_deep <- end_time - start_time
print(paste("Tiempo de compilación:", execution_time_deep))
```


**Prueba a modificar algunos de los parámetros tras haber ejecutado el código original y responde a las preguntas:**

-   ¿Qué cambios has hecho? ¿Por qué?
He realizado varios cambios a través del documento, que intente comentar en cada apartado o en comentarios, mi principal enfoque fue en reducir el tiempo de ejecución y la carga computacional sin comprometer la calidad de los resultados, por lo que reduje el número de variables a 100, el número de neuronas por capa a 100, el número de épocas a 500, el número de intentos aleatorios a 250, el tiempo máximo permitido para entrenar cada modelo a 100 minutos, el número de hilos de CPU utilizados por H2O a 6 y el tamaño máximo de memoria RAM asignada al servidor H2O a 12 GB.

además de esto realice divisiones más claras en las secciones de código para poder comentar más fácilmente y entender que funciones he modificado y capaz en un futuro me servirá modificar, y otras que deje tal cual el código original.

-   ¿Te ha surgido algún problema durante la ejecución del código?¿Cuál?¿Cómo lo has resuelto?
Teniendo en cuenta que mi portatíl junto a mi telefono son los únicos dispositivos que tengo para trabajar, y que el portatil es un dispositivo que uso para todo, el compilar un programa que consume el 100% del cpu, la relantice y me impidió hacer otras tareas, me pareció que no vale la pena por una tarea a la que probablemente estaría sobreentrenando, por eso decidí reducir la carga computacional y el tiempo de ejecución para no causarle daño a mi portatil, reduje el número de hilos de CPU utilizados por H2O a 6 y el tamaño máximo de memoria RAM asignada al servidor H2O a 12 GB, además de reducir el número de variables a 100 (que considero una gran mejora al simplificar el modelo, capaz no sé aproveche el gran dataset que se tiene, pero a veces las mejores soluciones nacen de la sencilles, y simplificar el modelo trajo decentes resultados), el tiempo máximo permitido para entrenar cada modelo a 300.

También tuve problemas al probar algunas variaciones como intentar realizar 10 repeticiones del 10-fold-CV, lo cual aumentaba demasiado el tiempo de ejecución a tal punto que no sé llego a terminar de compilar y tuve que hacer uso del equipo para ir a clase, por lo que lo reduje a 1 repetición, y al intentar reducir el número de intentos aleatorios (de `500` a `250`) para disminuir el ritmo de entrenamiento, aunque a diferencia de los otros puntos el reducir la cantidad de intentos redujo significativamente los resultados (por lo cual no se termino dejando este cambio).

-   ¿Has obtenido resultados significativamente distintos (acc, auc)?
Considero que no ha variado tanto como se esperaba, pero si se ha reducido el tiempo de ejecución y la carga computacional notoriamente, los resultados siguen siendo decentes, por lo que considero que los cambios realizados han sido positivos, capaz si la tarea hubiera sido enfocarse en minimizar los falsos positivos implementando otras métricas como el recall, hubiera sido más interesante el estudio de cambios de variables. 

**Opcional: con el paquete glmnet, incluye en el bucle el entrenamiento de una modelo de regressión con LASSO y evalúalo (debes haberlo hecho en Minería de datos):**

```{r lasso, warning=FALSE}

# Registra el tiempo antes de compilar el archivo RMarkdown
start_time <- Sys.time()

# Bucle sobre varias repeticiones de validación cruzada
for (rep in seq(REP.INIT, REP.END)) {
    cat(paste0("Repetition ", rep, "\n\n"))
    
    # Obtener los folds para validación cruzada de 10 pliegues
    folds <- datainfo$folds[datainfo$folds[,1] == rep, 2:ncol(datainfo$folds)]
    
    num.outter.folds <- ncol(folds)
    iter.res <- NULL
    start.time <- Sys.time()
    
    # Bucle sobre los diferentes folds
    for (ff in 1:num.outter.folds) {
        cat(paste0("CV ", ff, "-fold\n"))
        
        # Obtener los ids de entrenamiento y test
        train.ids <- which(folds[, ff] != -1)
        test.ids <- which(folds[, ff] == -1)
        X.train <- X[train.ids, ]
        X.test <- X[test.ids, ]
        Y.train <- y[train.ids]
        Y.test <- y[test.ids]
        
        # Inner folds para usar en glmnet y aprender el mejor lambda
        inner.folds <- folds[train.ids, ff]

        ########################################################################################
        # Selección univariada de características (opcional)
        retained.features <- ttest.feature.reduction(X.train, Y.train, pval.thres = PVAL.THRES, max.vars = MAX.VARS)
        print(paste0(length(retained.features), " genes retained..."))
        X.train <- X.train[, retained.features, drop = FALSE]
        X.test <- X.test[, retained.features, drop = FALSE]

        ########################################################################################
        # Ajustar el modelo LASSO usando validación cruzada interna para seleccionar el mejor lambda
        lasso_model <- cv.glmnet(X.train, Y.train, alpha = 1, family = "binomial", type.measure = "auc")
        best_lambda <- lasso_model$lambda.min  # Mejor valor de lambda

        # Guardar las características seleccionadas por LASSO
        non_zero_coeffs <- which(coef(lasso_model, s = "lambda.min") != 0) - 1  # Índices de características retenidas
        iter.res[[paste0("cvfold", ff)]][["retained.predictors"]] <- retained.features[non_zero_coeffs]

        # Predicciones en el conjunto de entrenamiento y test
        train.predictions <- predict(lasso_model, newx = X.train, s = "lambda.min", type = "response")
        test.predictions <- predict(lasso_model, newx = X.test, s = "lambda.min", type = "response")

        # Evaluar las métricas de clasificación
        train.auc <- get.classification.measures(true.class = Y.train, pred.probs = train.predictions)
        test.auc <- get.classification.measures(true.class = Y.test, pred.probs = test.predictions)

        # Guardar resultados
        iter.res[[paste0("cvfold", ff)]][["train.perf"]] <- train.auc
        iter.res[[paste0("cvfold", ff)]][["test.perf"]] <- test.auc

        # Guardar las métricas en la matriz de resultados
        perfs.mat[right.row, ] <- c(rep, train.auc, test.auc)
        print(perfs.mat[right.row, ])
        right.row <- right.row + 1

        # Guardar los resultados parciales en un CSV
        write.csv(perfs.mat, file = SAVE.CSV.FILE, quote = F, row.names = F)
    } # Fin del bucle de folds
    
    # Calcular el tiempo de simulación en minutos
    time.in.mins <- as.numeric(difftime(Sys.time(), start.time, units = "mins"))
    simul[[paste0("repetition", rep)]] <- list(folds = iter.res, time.in.mins = time.in.mins)
    
    # Guardar la simulación
    save(simul, file = SAVE.RDATA.FILE, compress = "xz")
} # Fin del bucle de repeticiones

# Registrar el tiempo después de la compilación
end_time <- Sys.time()

# Calcular y mostrar el tiempo de ejecución
execution_time_lasso <- end_time - start_time
print(paste("Tiempo de compilación (LASSO):", execution_time_lasso))


```


-   Compara los resultados obtenidos con LASSO con los que produce la red de aprendizaje profundo. ¿Es LASSO competitivo?¿Merece la pena el mayor tiempo de entrenamiento dedicado a la red?

En la comparación entre los resultados obtenidos con LASSO y los producidos por la red de aprendizaje profundo, se observa que ambos modelos son comparables en términos de rendimiento, con LASSO destacando por su notable rapidez de ejecución. Este factor hace que LASSO sea preferible en situaciones donde el tiempo de entrenamiento es crítico o cuando se trabaja con recursos computacionales limitados. Además, el hecho de que LASSO incorpore un mecanismo de selección de características lo convierte en una opción cómoda y eficiente, ya que no solo ajusta el modelo, sino que también reduce la dimensionalidad automáticamente, eliminando características irrelevantes o redundantes.

Sin embargo, aunque LASSO puede ser más rápido y eficiente en muchos casos, no debe descartarse el valor que puede aportar el aprendizaje profundo en ciertos contextos. Las redes profundas, aunque más costosas en términos de tiempo de entrenamiento, son capaces de capturar relaciones complejas y no lineales entre las variables de entrada, lo que puede ser esencial en problemas donde las características presentan interacciones intrincadas que un modelo lineal como LASSO no puede modelar adecuadamente.