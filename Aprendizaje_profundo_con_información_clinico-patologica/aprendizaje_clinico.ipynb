{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Aprendizaje profundo con información clínica patológica**\n",
        "\n",
        "Mi intención es facilitar lo mayor posible la corrección de este trabajo, siendo el apartado de tratamientos de datos sin cambiar tanto el código base del trabajo, pero agregando puntos que considero que facilitarán y mejoraran la creación de un modelo, estos puntos serán comentados y razonados en los bloques markdown que empiezan con **agregado**."
      ],
      "metadata": {
        "id": "CNb5oYwycI_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuKL1ld0Qczt"
      },
      "source": [
        "Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ts_8HYPQczu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # pip install seaborn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import RandomUnderSampler # pip install imbalanced-learn\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc, log_loss\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBxe54fxQczv"
      },
      "source": [
        "Lectura del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UKS4z1EQczv"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"mort_hospital.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-LhsBVQczw"
      },
      "source": [
        "**Problema**: Alto número de columnas con datos no informados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9x9MSHpQczw"
      },
      "outputs": [],
      "source": [
        "df.isnull().any().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l7-Fw_AQczw"
      },
      "source": [
        "**Problema**: Presencia de variables no numéricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVloEpOWQczw"
      },
      "outputs": [],
      "source": [
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "cols = df.select_dtypes([np.number]).columns\n",
        "print(\"Columnas con datos categóricos:\", len(object_columns))\n",
        "print(\"Columnas con datos numéricos:\", len(cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWQPX7lQczw"
      },
      "source": [
        "**Problema**: Las clases de la variable de interés están muy desequilibradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzK9d7scQczw"
      },
      "outputs": [],
      "source": [
        "sns.set_style('whitegrid')\n",
        "ax = sns.countplot(x='hospital_death', data=df)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agregado:** El plot muestra ahora exactamente el número exacto de cada clase.\n",
        "\n",
        "A continuación cree una función que estudia cuanto porcentaje de datos faltantes hay en cada variable, puesto que casos como `h1_bilirubin_min` que muestra un 92,27% de datos faltantes, rellenarlos con el promedio (por temas de legibilidad del markdown no puse los códigos donde evalue cada variable por separado, pero todas las que tienen más de un 50% de datos faltantes eran variables númericas), considero que esto en vez de ayudar a encontrar patrones en los datos con los que predecir puede causar sobrentrenamiento o percepción de patrones inexistentes.\n",
        "\n",
        "Por lo que se uso un limite de aquellas variables con más de 50% datos faltantes serán borrados."
      ],
      "metadata": {
        "id": "3wuKrmqGR4Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eliminar_columnas_con_muchos_na(df, porcentaje_limite=50):\n",
        "    \"\"\"\n",
        "    Estudia el porcentaje de valores NA en cada columna de un DataFrame y elimina las columnas que superen el porcentaje límite.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame de pandas.\n",
        "        porcentaje_limite: Porcentaje límite de valores NA para eliminar una columna.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame de pandas con las columnas eliminadas o el mismo DataFrame si no se eliminaron columnas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calcular el porcentaje de valores NA en cada columna\n",
        "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Ordenar las columnas por porcentaje de NA de mayor a menor\n",
        "    missing_series = missing_percentage.sort_values(ascending=False)\n",
        "\n",
        "    # Identificar las columnas a eliminar\n",
        "    columnas_a_eliminar = missing_series[missing_series > porcentaje_limite].index\n",
        "\n",
        "    # Eliminar las columnas si hay alguna que supere el límite\n",
        "    if len(columnas_a_eliminar) > 0:\n",
        "        print(\"Columnas eliminadas por superar el límite de NA:\")\n",
        "        for column, percentage in missing_series[missing_series > porcentaje_limite].items():\n",
        "            print(f\"{column}: {percentage:.2f}%\")\n",
        "\n",
        "        print(\"\\nColumnas que permanecen en el dataset:\")\n",
        "        for column, percentage in missing_series[missing_series <= porcentaje_limite].items():\n",
        "            print(f\"{column}: {percentage:.2f}%\")\n",
        "\n",
        "        df = df.drop(columns=columnas_a_eliminar)\n",
        "    else:\n",
        "        print(\"Todas las columnas tienen un porcentaje de NA aceptable.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "df = eliminar_columnas_con_muchos_na(df,25)"
      ],
      "metadata": {
        "id": "i6BYtqfYVMiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agregado:** Recapitulación de información de las columnas, actualmente el dataframe cuenta con 70 columnas numéricas menos."
      ],
      "metadata": {
        "id": "dIHrQ6fxdzv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "cols = df.select_dtypes([np.number]).columns\n",
        "print(\"Columnas con datos categóricos:\", len(object_columns))\n",
        "print(\"Columnas con datos numéricos:\", len(cols))"
      ],
      "metadata": {
        "id": "nsJnoYa6d1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78KyJSF-Qczw"
      },
      "source": [
        "Solución al problema de los datos no informados:\n",
        "- Variables categóricas: rellenar con la moda\n",
        "- Variables numéricas: rellenar con el promedio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-6JmQNwQczx"
      },
      "outputs": [],
      "source": [
        "for i in object_columns:\n",
        "    df[i].fillna(df[i].mode()[0], inplace=True)\n",
        "\n",
        "df[cols] = df[cols].fillna(df[cols].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNo52_9CQczx"
      },
      "source": [
        "Solución al problema de las variables no numéricas:\n",
        "- Codificación de variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6GYgWCzQczx"
      },
      "outputs": [],
      "source": [
        "print(\"Valores de la variable 'etnicity' antes de la codificación:\", df['ethnicity'].unique())\n",
        "le = LabelEncoder()\n",
        "df[object_columns] = df[object_columns].apply(le.fit_transform)\n",
        "print(\"Valores de la variable 'etnicity' después de la codificación:\", df['ethnicity'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_l67OJIQczx"
      },
      "source": [
        "Solución al problema del desequilibrio de la variable de interés:\n",
        "- Muestreo de la clase más numerosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rKgTn0cQczx"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns='hospital_death', axis=1)\n",
        "y = df['hospital_death']\n",
        "\n",
        "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "X, y = undersample.fit_resample(X, y)\n",
        "\n",
        "print(\"Recuento de categorías de salida:\", Counter(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sd80fc5Qczx"
      },
      "source": [
        "Selección de características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB8gAS1EQczx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4QazySPQczx"
      },
      "source": [
        "Normalización de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considero que el trabajo real empieza a partir de este punto, si quieres confirmar que mi entendimiento del código proporcionado puedes ver [mi repositorio github que hablo más a detalle en el readme](https://github.com/Diegodepab/Redes_aprendizaje_python_y_R/tree/main/Aprendizaje_profundo_con_informaci%C3%B3n_clinico-patologica) sobre el preprocesamiento de datos."
      ],
      "metadata": {
        "id": "DWsn8xPUq6Vh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sM1y-7ZQczx"
      },
      "source": [
        "### Solución de la tarea 2\n",
        "**Objetivos**\n",
        "- Entender el código proporcionado: tratamiento de datos no informados, codificación de variables categóricas, equilibrado de categorías de la variable de interés, selección de características y normalización.\n",
        "- Diseñar y entrenar una red neuronal para predecir la variable 'hospital_death':\n",
        "    - Probar con distintos valores de 'num_k' para cambiar el número de características seleccionadas.\n",
        "    - Diseñar un modelo secuencial ('Sequential()'), cuya primera capa debe especificar como 'input_dim' el valor 'num_k', probando distintos números de capas, funciones de activación (RELU y variantes para las capas ocultas, sigmoide para la capa final), número de neuronas en cada capa, capas 'Dropout', etc.\n",
        "    - Compilar el modelo estableciendo una función de error ('loss'), el optimizador (puedes cambiar el parámetro 'learning_rate') y las métricas de rendimiento que se calcularán.\n",
        "    - Ajustar el modelo con los datos de entrenamiento estableciendo para la función 'fit' el número de iteraciones ('epochs'), el porcentaje de datos de validación ('validation_split') y el tamaño de lote para la retropropagación ('batch_size').\n",
        "- **OPCIONAL**: En lugar de cablear en el código los parámetros anteriores y modificarlos a mano, se valorará muy positivamente la escritura de un programa que genere dinámicamente diferentes arquitecturas probando distintos valores para todos o algunos de los parámetros del punto anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voy a plantear dos funciones para cumplir los objetivos deseados. La primera función probará distintos valores de num_k y transformará los datos en función de los mejores atributos seleccionados. La segunda diseñará el modelo secuencial con diferentes configuraciones, entrenará, y mostrará tanto la curva ROC como la precisión para determinar el mejor num_k.\n",
        "\n",
        "###  Primera función: Selección de características\n",
        "\n",
        "Antes de entrenar el modelo, es importante realizar una **selección adecuada de las características más relevantes** del conjunto de datos para mejorar el rendimiento del modelo. En este caso, se utilizará la función `SelectKBest` de `sklearn`, que selecciona las mejores características basándose en una métrica de información mutua. El parámetro k define cuántas características se deben seleccionar, y se probarán diferentes valores de k para encontrar la cantidad óptima de características.\n",
        "\n",
        "El código que se presenta a continuación permite transformar los conjuntos de **entrenamiento** y **prueba** seleccionando las mejores características según los valores de k definidos. Posteriormente, los datos son estandarizados usando `StandardScaler` para asegurar que todas las características tengan la misma escala, lo que es fundamental para el buen desempeño del modelo. El resultado es un diccionario que contiene las transformaciones para cada valor de k, que luego se pueden usar en el entrenamiento y evaluación del modelo."
      ],
      "metadata": {
        "id": "-4UmJD2it5gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seleccionar_mejores_k(X_train, X_test, y_train, num_ks):\n",
        "    resultados = {}\n",
        "\n",
        "    for num_k in num_ks:\n",
        "        # Seleccionar las mejores k características\n",
        "        selector = SelectKBest(mutual_info_classif, k=num_k)\n",
        "        selector.fit(X_train, y_train)\n",
        "\n",
        "        # Transformar los datos de entrenamiento y prueba\n",
        "        X_train_new = selector.transform(X_train)\n",
        "        X_test_new = selector.transform(X_test)\n",
        "\n",
        "        # Estandarización de los datos\n",
        "        scaler = StandardScaler()\n",
        "        X_train_std = scaler.fit_transform(X_train_new)\n",
        "        X_test_std = scaler.transform(X_test_new)\n",
        "\n",
        "        # Almacenar los datos transformados\n",
        "        resultados[num_k] = (X_train_std, X_test_std)\n",
        "\n",
        "    return resultados\n",
        "\n",
        "num_ks = [5, 10, 15, 20, 22, 25, 30, 35]\n",
        "resultados_k = seleccionar_mejores_k(X_train, X_test, y_train, num_ks)\n",
        "#print(resultados_k)"
      ],
      "metadata": {
        "id": "1uFFrxm_rxe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado mostrado es un diccionario en el que cada clave representa un valor de `\"k\"`, que indica el número de características seleccionadas en el proceso de selección de las mejores características para el modelo. Para cada valor de `\"k\"`, se almacenan dos matrices: una para los datos de entrenamiento (`X_train_std`) y otra para los datos de prueba (`X_test_std`). Estas matrices contienen los datos transformados, donde las características seleccionadas han sido normalizadas (escaladas) para que tengan una media de 0 y una desviación estándar de 1. Esto permite que el modelo sea entrenado de manera más eficiente, eliminando el impacto de las diferentes escalas de las características originales. Cada conjunto de datos corresponde a una selección diferente de características según el valor de \"`k`\", lo que permite comparar cómo el número de características seleccionadas afecta al desempeño del modelo.\n",
        "\n",
        "###  Evaluación de `num_k` y diseño de modelos\n",
        "\n",
        "A continuación se lleva a cabo la evaluación del desempeño de **los modelos secuenciales** generados con diferentes valores de `num_k` anteriormente. Se utiliza un ciclo para probar cada conjunto de características seleccionadas, diseñando un modelo de red neuronal secuencial con diferentes capas, funciones de activación (ReLU para las capas ocultas y sigmoide para la capa de salida), y la inclusión de capas Dropout para evitar el sobreajuste.\n",
        "\n",
        "El modelo se compila con una función de error (`binary_crossentropy`), un optimizador `Adam` con un `learning_rate` ajustable y se entrena con un número de épocas definidas y un tamaño de lote específico. Además, la función permite usar validación cruzada (Que se usará más adelante con los mejores resultados) para obtener una evaluación más robusta mediante la métrica ROC-AUC. El rendimiento de cada modelo se muestra a través de las métricas de ROC-AUC y precisión (accuracy). La función retorna el valor de num_k que ha mostrado el mejor desempeño en cuanto a ROC-AUC."
      ],
      "metadata": {
        "id": "ucJyk2rBr4zg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BVTTYgpQczy",
        "outputId": "db91d35b-391b-4d37-935f-086ccf8d76cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "num_k: 5, ROC-AUC: 0.8623, Accuracy: 0.7720\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "num_k: 10, ROC-AUC: 0.8747, Accuracy: 0.7953\n"
          ]
        }
      ],
      "source": [
        "def evaluar_k_modelos(resultados_k, y_train, y_test, use_cross_validation=False):\n",
        "    mejor_k = None\n",
        "    mejor_roc_auc = 0\n",
        "\n",
        "    for num_k, (X_train_std, X_test_std) in resultados_k.items():\n",
        "        # Diseñar el modelo secuencial\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=num_k, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Compilación\n",
        "        optimizer = Adam(learning_rate=0.001) #mejor learning rate conseguido en otras compilaciones\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        if use_cross_validation:\n",
        "            # Validación cruzada\n",
        "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            scores = cross_val_score(model, X_train_std, y_train, cv=kfold, scoring='roc_auc')\n",
        "            roc_auc = scores.mean()\n",
        "        else:\n",
        "            # Entrenamiento sin validación cruzada\n",
        "            model.fit(X_train_std, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "            y_pred = model.predict(X_test_std).ravel()\n",
        "            roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
        "\n",
        "        print(f\"num_k: {num_k}, ROC-AUC: {roc_auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Guardar el mejor k\n",
        "        if roc_auc > mejor_roc_auc:\n",
        "            mejor_roc_auc = roc_auc\n",
        "            mejor_k = num_k\n",
        "    return mejor_k\n",
        "\n",
        "mejor_k = evaluar_k_modelos(resultados_k, y_train, y_test, use_cross_validation=False)\n",
        "print(f\"El mejor valor de num_k es: {mejor_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya una vez conseguido el mejor `num_k` se óptará por probar las mejores configuraciones para crear y entrenar un modelo:"
      ],
      "metadata": {
        "id": "QIGt4sho-cDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función capaz de crear y entrenar el modelo\n",
        "\n",
        "\n",
        "La función `create_and_train_model` crea y entrena un modelo de red neuronal utilizando la configuración de capas proporcionada (`layers_config`) y otros parámetros como la tasa de aprendizaje, la tasa de dropout, el número de épocas y el tamaño de lote. El modelo usa capas densas con activación `ReLU` para las *capas ocultas* y `sigmoide` para la* capa de salida,* aplicando *Dropout* para evitar el sobreajuste. También incorpora `EarlyStopping` para interrumpir el entrenamiento si no hay mejoras en la validación. Esta función se usará en otro bloque para probar diferentes configuraciones de redes neuronales."
      ],
      "metadata": {
        "id": "r7r4eWCu8qT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_model(X_train, y_train, X_val, y_val, num_k, layers, dropout_rate, learning_rate, epochs, batch_size, validation_split):\n",
        "    # Crear el modelo secuencial\n",
        "    model = Sequential()\n",
        "\n",
        "    # Agregar capas ocultas\n",
        "    for units in layers:\n",
        "        model.add(Dense(units, activation='relu', input_dim=num_k))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Agregar capa de salida\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        verbose=0  # Cambiar a 1 si se desea ver el progreso\n",
        "    )\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de validación\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "    return model, val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "kVQtsP2I8q29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probar diferentes configuraciones\n",
        "\n",
        "La función `optimize_neural_network` busca encontrar la mejor configuración para entrenar una *red neuronal*, probando todas las combinaciones posibles de parámetros de la red. Primero, normaliza los datos utilizando `StandardScaler` y los divide en conjuntos de entrenamiento y validación. Luego, itera a través de diferentes configuraciones de la red, como el número de capas, la tasa de dropout, la tasa de aprendizaje y el tamaño del lote. Para cada combinación de estos parámetros, entrena el modelo y evalúa su rendimiento en el conjunto de validación. Al final, guarda y devuelve la configuración que logró la mejor precisión en validación. Esta función es útil para realizar una búsqueda exhaustiva de hiperparámetros para optimizar el rendimiento del modelo."
      ],
      "metadata": {
        "id": "tTvbpWmV8sq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def optimize_neural_network(X, y, num_k, config_options):\n",
        "    # Normalizar datos\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Seleccionar las primeras num_k características\n",
        "    X_scaled = X_scaled[:, :num_k]\n",
        "\n",
        "    # Dividir los datos en conjunto de entrenamiento y validación\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_config = None\n",
        "\n",
        "    # Búsqueda de todas las combinaciones posibles\n",
        "    for layers in config_options['layers']:\n",
        "        for dropout_rate in config_options['dropout_rates']:\n",
        "            for learning_rate in config_options['learning_rates']:\n",
        "                for batch_size in config_options['batch_sizes']:\n",
        "                    for epochs in config_options['epochs']:\n",
        "                        for validation_split in config_options['validation_splits']:\n",
        "                            print(f\"Probando configuración: Layers: {layers}, Dropout: {dropout_rate}, \"\n",
        "                                  f\"Learning Rate: {learning_rate}, Batch Size: {batch_size}, \"\n",
        "                                  f\"Epochs: {epochs}, Validation Split: {validation_split}\")\n",
        "\n",
        "                            # Entrenar el modelo con la configuración actual\n",
        "                            model, val_loss, val_accuracy = create_and_train_model(\n",
        "                                X_train, y_train, X_val, y_val, num_k, layers,\n",
        "                                dropout_rate, learning_rate, epochs, batch_size, validation_split\n",
        "                            )\n",
        "\n",
        "                            # Guardar la mejor configuración\n",
        "                            if val_accuracy > best_accuracy:\n",
        "                                best_accuracy = val_accuracy\n",
        "                                best_model = model\n",
        "                                best_config = (layers, dropout_rate, learning_rate, batch_size, epochs, validation_split)\n",
        "\n",
        "    print(f\"Mejor configuración encontrada: {best_config} con precisión de validación: {best_accuracy}\")\n",
        "\n",
        "    return best_model, best_config, best_accuracy\n",
        "\n",
        "# Configuración de hiperparámetros\n",
        "config_options = {\n",
        "    'layers': [[128, 64], [256, 128]], #Descartados por tiempo d ecompilacion muy largos y resultados no tan prometedores para una validacion_ , [64, 32]\n",
        "    'dropout_rates': [0.3, 0.4], #, 0.5\n",
        "    'learning_rates': [0.001], # , 0.0005, 0.1, 0.01\n",
        "    'batch_sizes': [32, 64],\n",
        "    'epochs': [100, 150], #50,\n",
        "    'validation_splits': [0.2, 0.3] #0.1,\n",
        "}\n",
        "\n",
        "best_model, best_config, best_accuracy = optimize_neural_network(\n",
        "    X, y, mejor_k, config_options=config_options\n",
        ")\n"
      ],
      "metadata": {
        "id": "I8BGZpBu8s91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados finales"
      ],
      "metadata": {
        "id": "kuYM3Cn-ssIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_k, layers, dropout_rate, learning_rate):\n",
        "    # Crear el modelo secuencial\n",
        "    model = Sequential()\n",
        "\n",
        "    # Agregar capas ocultas\n",
        "    for units in layers:\n",
        "        model.add(Dense(units, activation='relu', input_dim=num_k))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Agregar capa de salida\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_with_cross_validation(X, y, num_k, best_config, n_splits=5):\n",
        "    # Desempaquetar la mejor configuración\n",
        "    layers, dropout_rate, learning_rate, batch_size, epochs, _ = best_config\n",
        "\n",
        "    # Normalizar datos\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Seleccionar las primeras num_k características\n",
        "    X_scaled = X_scaled[:, :num_k]\n",
        "\n",
        "    # Configurar KFold para validación cruzada\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=2)\n",
        "\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    # Validación cruzada\n",
        "    for train_index, val_index in kf.split(X_scaled):\n",
        "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Crear modelo\n",
        "        model = create_model(num_k, layers, dropout_rate, learning_rate)\n",
        "\n",
        "        # Entrenar modelo\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Evaluar modelo\n",
        "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        losses.append(val_loss)\n",
        "        accuracies.append(val_accuracy)\n",
        "\n",
        "        # Predecir probabilidades para la curva ROC\n",
        "        y_pred_prob = model.predict(X_val).ravel()\n",
        "        all_y_true.extend(y_val)\n",
        "        all_y_pred.extend(y_pred_prob)\n",
        "\n",
        "    # Calcular métricas promedio\n",
        "    mean_accuracy = np.mean(accuracies)\n",
        "    mean_loss = np.mean(losses)\n",
        "\n",
        "    # Generar curva ROC\n",
        "    fpr, tpr, _ = roc_curve(all_y_true, all_y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Mean Accuracy: {mean_accuracy:.4f}')\n",
        "    print(f'Mean Loss: {mean_loss:.4f}')\n",
        "    print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "#########################################################################\n",
        "evaluate_with_cross_validation(X, y, mejor_k, best_config)\n"
      ],
      "metadata": {
        "id": "yNhauzjKssaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}